{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quocnle/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Cifar10\n",
      "INFO:tensorflow:loading file: data/cifar-10-batches-py/data_batch_1\n",
      "INFO:tensorflow:loading file: data/cifar-10-batches-py/data_batch_2\n",
      "INFO:tensorflow:loading file: data/cifar-10-batches-py/data_batch_3\n",
      "INFO:tensorflow:loading file: data/cifar-10-batches-py/data_batch_4\n",
      "INFO:tensorflow:loading file: data/cifar-10-batches-py/data_batch_5\n",
      "INFO:tensorflow:loading file: data/cifar-10-batches-py/test_batch\n",
      "INFO:tensorflow:mean:[0.49139968, 0.48215841, 0.44653091]    std: [0.24703223, 0.24348513, 0.26158784]\n",
      "INFO:tensorflow:In CIFAR10 loader, number of images: 60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quocnle/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:92: DeprecationWarning: With-statements now directly support multiple context managers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:number of trainable params: 36220634\n",
      "INFO:tensorflow:number of trainable params: 36220634\n",
      "INFO:tensorflow:Saved child model\n",
      "INFO:tensorflow:Evaluating child model in mode val\n",
      "INFO:tensorflow:Restoring parameters from /tmp/training/model/model.ckpt-0\n",
      "INFO:tensorflow:Loaded child model checkpoint from /tmp/training/model/model.ckpt-0\n",
      "INFO:tensorflow:model.batch_size is 25\n",
      "INFO:tensorflow:Eval child model accuracy: 0.0\n",
      "INFO:tensorflow:Before Training Epoch: 0     Val Acc: 0.0\n",
      "INFO:tensorflow:Restoring parameters from /tmp/training/model/model.ckpt-0\n",
      "INFO:tensorflow:Loaded child model checkpoint from /tmp/training/model/model.ckpt-0\n",
      "INFO:tensorflow:steps per epoch: 390\n",
      "INFO:tensorflow:lr of 0.1 for epoch 0\n",
      "INFO:tensorflow:Training 0/390\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2018 The TensorFlow Authors All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"AutoAugment Train/Eval module.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import time\n",
    "\n",
    "import custom_ops as ops\n",
    "import data_utils\n",
    "import helper_utils\n",
    "import numpy as np\n",
    "from shake_drop import build_shake_drop_model\n",
    "from shake_shake import build_shake_shake_model\n",
    "import tensorflow as tf\n",
    "from wrn import build_wrn_model\n",
    "\n",
    "tf.flags.DEFINE_string('model_name', 'wrn',\n",
    "                       'wrn, shake_shake_32, shake_shake_96, shake_shake_112, '\n",
    "                       'pyramid_net')\n",
    "tf.flags.DEFINE_string('checkpoint_dir', '/tmp/training', 'Training Directory.')\n",
    "tf.flags.DEFINE_string('data_path', 'data/cifar-10-batches-py',\n",
    "                       'Directory where dataset is located.')\n",
    "tf.flags.DEFINE_string('dataset', 'cifar10',\n",
    "                       'Dataset to train with. Either cifar10 or cifar100')\n",
    "tf.flags.DEFINE_integer('use_cpu', 1, '1 if use CPU, else GPU.')\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "arg_scope = tf.contrib.framework.arg_scope\n",
    "\n",
    "\n",
    "def setup_arg_scopes(is_training):\n",
    "  \"\"\"Sets up the argscopes that will be used when building an image model.\n",
    "\n",
    "  Args:\n",
    "    is_training: Is the model training or not.\n",
    "\n",
    "  Returns:\n",
    "    Arg scopes to be put around the model being constructed.\n",
    "  \"\"\"\n",
    "\n",
    "  batch_norm_decay = 0.9\n",
    "  batch_norm_epsilon = 1e-5\n",
    "  batch_norm_params = {\n",
    "      # Decay for the moving averages.\n",
    "      'decay': batch_norm_decay,\n",
    "      # epsilon to prevent 0s in variance.\n",
    "      'epsilon': batch_norm_epsilon,\n",
    "      'scale': True,\n",
    "      # collection containing the moving mean and moving variance.\n",
    "      'is_training': is_training,\n",
    "  }\n",
    "\n",
    "  scopes = []\n",
    "\n",
    "  scopes.append(arg_scope([ops.batch_norm], **batch_norm_params))\n",
    "  return scopes\n",
    "\n",
    "\n",
    "def build_model(inputs, num_classes, is_training, hparams):\n",
    "  \"\"\"Constructs the vision model being trained/evaled.\n",
    "\n",
    "  Args:\n",
    "    inputs: input features/images being fed to the image model build built.\n",
    "    num_classes: number of output classes being predicted.\n",
    "    is_training: is the model training or not.\n",
    "    hparams: additional hyperparameters associated with the image model.\n",
    "\n",
    "  Returns:\n",
    "    The logits of the image model.\n",
    "  \"\"\"\n",
    "  scopes = setup_arg_scopes(is_training)\n",
    "  with contextlib.nested(*scopes):\n",
    "    if hparams.model_name == 'pyramid_net':\n",
    "      logits = build_shake_drop_model(\n",
    "          inputs, num_classes, is_training)\n",
    "    elif hparams.model_name == 'wrn':\n",
    "      logits = build_wrn_model(\n",
    "          inputs, num_classes, hparams.wrn_size)\n",
    "    elif hparams.model_name == 'shake_shake':\n",
    "      logits = build_shake_shake_model(\n",
    "          inputs, num_classes, hparams, is_training)\n",
    "  return logits\n",
    "\n",
    "\n",
    "class CifarModel(object):\n",
    "  \"\"\"Builds an image model for Cifar10/Cifar100.\"\"\"\n",
    "\n",
    "  def __init__(self, hparams):\n",
    "    self.hparams = hparams\n",
    "\n",
    "  def build(self, mode):\n",
    "    \"\"\"Construct the cifar model.\"\"\"\n",
    "    assert mode in ['train', 'eval']\n",
    "    self.mode = mode\n",
    "    self._setup_misc(mode)\n",
    "    self._setup_images_and_labels()\n",
    "    self._build_graph(self.images, self.labels, mode)\n",
    "\n",
    "    self.init = tf.group(tf.global_variables_initializer(),\n",
    "                         tf.local_variables_initializer())\n",
    "\n",
    "  def _setup_misc(self, mode):\n",
    "    \"\"\"Sets up miscellaneous in the cifar model constructor.\"\"\"\n",
    "    self.lr_rate_ph = tf.Variable(0.0, name='lrn_rate', trainable=False)\n",
    "    self.reuse = None if (mode == 'train') else True\n",
    "    self.batch_size = self.hparams.batch_size\n",
    "    if mode == 'eval':\n",
    "      self.batch_size = 25\n",
    "\n",
    "  def _setup_images_and_labels(self):\n",
    "    \"\"\"Sets up image and label placeholders for the cifar model.\"\"\"\n",
    "    if FLAGS.dataset == 'cifar10':\n",
    "      self.num_classes = 10\n",
    "    else:\n",
    "      self.num_classes = 100\n",
    "    self.images = tf.placeholder(tf.float32, [self.batch_size, 32, 32, 3])\n",
    "    self.labels = tf.placeholder(tf.float32,\n",
    "                                 [self.batch_size, self.num_classes])\n",
    "\n",
    "  def assign_epoch(self, session, epoch_value):\n",
    "    session.run(self._epoch_update, feed_dict={self._new_epoch: epoch_value})\n",
    "\n",
    "  def _build_graph(self, images, labels, mode):\n",
    "    \"\"\"Constructs the TF graph for the cifar model.\n",
    "\n",
    "    Args:\n",
    "      images: A 4-D image Tensor\n",
    "      labels: A 2-D labels Tensor.\n",
    "      mode: string indicating training mode ( e.g., 'train', 'valid', 'test').\n",
    "    \"\"\"\n",
    "    is_training = 'train' in mode\n",
    "    if is_training:\n",
    "      self.global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    logits = build_model(\n",
    "        images,\n",
    "        self.num_classes,\n",
    "        is_training,\n",
    "        self.hparams)\n",
    "    self.predictions, self.cost = helper_utils.setup_loss(\n",
    "        logits, labels)\n",
    "    self.accuracy, self.eval_op = tf.metrics.accuracy(\n",
    "        tf.argmax(labels, 1), tf.argmax(self.predictions, 1))\n",
    "    self._calc_num_trainable_params()\n",
    "\n",
    "    # Adds L2 weight decay to the cost\n",
    "    self.cost = helper_utils.decay_weights(self.cost,\n",
    "                                           self.hparams.weight_decay_rate)\n",
    "\n",
    "    if is_training:\n",
    "      self._build_train_op()\n",
    "\n",
    "    # Setup checkpointing for this child model\n",
    "    # Keep 2 or more checkpoints around during training.\n",
    "    with tf.device('/cpu:0'):\n",
    "      self.saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "    self.init = tf.group(tf.global_variables_initializer(),\n",
    "                         tf.local_variables_initializer())\n",
    "\n",
    "  def _calc_num_trainable_params(self):\n",
    "    self.num_trainable_params = np.sum([\n",
    "        np.prod(var.get_shape().as_list()) for var in tf.trainable_variables()\n",
    "    ])\n",
    "    tf.logging.info('number of trainable params: {}'.format(\n",
    "        self.num_trainable_params))\n",
    "\n",
    "  def _build_train_op(self):\n",
    "    \"\"\"Builds the train op for the cifar model.\"\"\"\n",
    "    hparams = self.hparams\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads = tf.gradients(self.cost, tvars)\n",
    "    if hparams.gradient_clipping_by_global_norm > 0.0:\n",
    "      grads, norm = tf.clip_by_global_norm(\n",
    "          grads, hparams.gradient_clipping_by_global_norm)\n",
    "      tf.summary.scalar('grad_norm', norm)\n",
    "\n",
    "    # Setup the initial learning rate\n",
    "    initial_lr = self.lr_rate_ph\n",
    "    optimizer = tf.train.MomentumOptimizer(\n",
    "        initial_lr,\n",
    "        0.9,\n",
    "        use_nesterov=True)\n",
    "\n",
    "    self.optimizer = optimizer\n",
    "    apply_op = optimizer.apply_gradients(\n",
    "        zip(grads, tvars), global_step=self.global_step, name='train_step')\n",
    "    train_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies([apply_op]):\n",
    "      self.train_op = tf.group(*train_ops)\n",
    "\n",
    "\n",
    "class CifarModelTrainer(object):\n",
    "  \"\"\"Trains an instance of the CifarModel class.\"\"\"\n",
    "\n",
    "  def __init__(self, hparams):\n",
    "    self._session = None\n",
    "    self.hparams = hparams\n",
    "\n",
    "    self.model_dir = os.path.join(FLAGS.checkpoint_dir, 'model')\n",
    "    self.log_dir = os.path.join(FLAGS.checkpoint_dir, 'log')\n",
    "    # Set the random seed to be sure the same validation set\n",
    "    # is used for each model\n",
    "    np.random.seed(0)\n",
    "    self.data_loader = data_utils.DataSet(hparams)\n",
    "    np.random.seed()  # Put the random seed back to random\n",
    "    self.data_loader.reset()\n",
    "\n",
    "  def save_model(self, step=None):\n",
    "    \"\"\"Dumps model into the backup_dir.\n",
    "\n",
    "    Args:\n",
    "      step: If provided, creates a checkpoint with the given step\n",
    "        number, instead of overwriting the existing checkpoints.\n",
    "    \"\"\"\n",
    "    model_save_name = os.path.join(self.model_dir, 'model.ckpt')\n",
    "    if not tf.gfile.IsDirectory(self.model_dir):\n",
    "      tf.gfile.MakeDirs(self.model_dir)\n",
    "    self.saver.save(self.session, model_save_name, global_step=step)\n",
    "    tf.logging.info('Saved child model')\n",
    "\n",
    "  def extract_model_spec(self):\n",
    "    \"\"\"Loads a checkpoint with the architecture structure stored in the name.\"\"\"\n",
    "    checkpoint_path = tf.train.latest_checkpoint(self.model_dir)\n",
    "    if checkpoint_path is not None:\n",
    "      self.saver.restore(self.session, checkpoint_path)\n",
    "      tf.logging.info('Loaded child model checkpoint from %s',\n",
    "                      checkpoint_path)\n",
    "    else:\n",
    "      self.save_model(step=0)\n",
    "\n",
    "  def eval_child_model(self, model, data_loader, mode):\n",
    "    \"\"\"Evaluate the child model.\n",
    "\n",
    "    Args:\n",
    "      model: image model that will be evaluated.\n",
    "      data_loader: dataset object to extract eval data from.\n",
    "      mode: will the model be evalled on train, val or test.\n",
    "\n",
    "    Returns:\n",
    "      Accuracy of the model on the specified dataset.\n",
    "    \"\"\"\n",
    "    tf.logging.info('Evaluating child model in mode %s', mode)\n",
    "    while True:\n",
    "      try:\n",
    "        with self._new_session(model):\n",
    "          accuracy = helper_utils.eval_child_model(\n",
    "              self.session,\n",
    "              model,\n",
    "              data_loader,\n",
    "              mode)\n",
    "          tf.logging.info('Eval child model accuracy: {}'.format(accuracy))\n",
    "          # If epoch trained without raising the below errors, break\n",
    "          # from loop.\n",
    "          break\n",
    "      except (tf.errors.AbortedError, tf.errors.UnavailableError) as e:\n",
    "        tf.logging.info('Retryable error caught: %s.  Retrying.', e)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "  @contextlib.contextmanager\n",
    "  def _new_session(self, m):\n",
    "    \"\"\"Creates a new session for model m.\"\"\"\n",
    "    # Create a new session for this model, initialize\n",
    "    # variables, and save / restore from\n",
    "    # checkpoint.\n",
    "    self._session = tf.Session(\n",
    "        '',\n",
    "        config=tf.ConfigProto(\n",
    "            allow_soft_placement=True, log_device_placement=False))\n",
    "    self.session.run(m.init)\n",
    "\n",
    "    # Load in a previous checkpoint, or save this one\n",
    "    self.extract_model_spec()\n",
    "    try:\n",
    "      yield\n",
    "    finally:\n",
    "      tf.Session.reset('')\n",
    "      self._session = None\n",
    "\n",
    "  def _build_models(self):\n",
    "    \"\"\"Builds the image models for train and eval.\"\"\"\n",
    "    # Determine if we should build the train and eval model. When using\n",
    "    # distributed training we only want to build one or the other and not both.\n",
    "    with tf.variable_scope('model', use_resource=False):\n",
    "      m = CifarModel(self.hparams)\n",
    "      m.build('train')\n",
    "      self._num_trainable_params = m.num_trainable_params\n",
    "      self._saver = m.saver\n",
    "    with tf.variable_scope('model', reuse=True, use_resource=False):\n",
    "      meval = CifarModel(self.hparams)\n",
    "      meval.build('eval')\n",
    "    return m, meval\n",
    "\n",
    "  def _calc_starting_epoch(self, m):\n",
    "    \"\"\"Calculates the starting epoch for model m based on global step.\"\"\"\n",
    "    hparams = self.hparams\n",
    "    batch_size = hparams.batch_size\n",
    "    steps_per_epoch = int(hparams.train_size / batch_size)\n",
    "    with self._new_session(m):\n",
    "      curr_step = self.session.run(m.global_step)\n",
    "    total_steps = steps_per_epoch * hparams.num_epochs\n",
    "    epochs_left = (total_steps - curr_step) // steps_per_epoch\n",
    "    starting_epoch = hparams.num_epochs - epochs_left\n",
    "    return starting_epoch\n",
    "\n",
    "  def _run_training_loop(self, m, curr_epoch):\n",
    "    \"\"\"Trains the cifar model `m` for one epoch.\"\"\"\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "      try:\n",
    "        with self._new_session(m):\n",
    "          train_accuracy = helper_utils.run_epoch_training(\n",
    "              self.session, m, self.data_loader, curr_epoch)\n",
    "          tf.logging.info('Saving model after epoch')\n",
    "          self.save_model(step=curr_epoch)\n",
    "          break\n",
    "      except (tf.errors.AbortedError, tf.errors.UnavailableError) as e:\n",
    "        tf.logging.info('Retryable error caught: %s.  Retrying.', e)\n",
    "    tf.logging.info('Finished epoch: {}'.format(curr_epoch))\n",
    "    tf.logging.info('Epoch time(min): {}'.format(\n",
    "        (time.time() - start_time) / 60.0))\n",
    "    return train_accuracy\n",
    "\n",
    "  def _compute_final_accuracies(self, meval):\n",
    "    \"\"\"Run once training is finished to compute final val/test accuracies.\"\"\"\n",
    "    valid_accuracy = self.eval_child_model(meval, self.data_loader, 'val')\n",
    "    if self.hparams.eval_test:\n",
    "      test_accuracy = self.eval_child_model(meval, self.data_loader, 'test')\n",
    "    else:\n",
    "      test_accuracy = 0\n",
    "    tf.logging.info('Test Accuracy: {}'.format(test_accuracy))\n",
    "    return valid_accuracy, test_accuracy\n",
    "\n",
    "  def run_model(self):\n",
    "    \"\"\"Trains and evalutes the image model.\"\"\"\n",
    "    hparams = self.hparams\n",
    "\n",
    "    # Build the child graph\n",
    "    with tf.Graph().as_default(), tf.device(\n",
    "        '/cpu:0' if FLAGS.use_cpu else '/gpu:0'):\n",
    "      m, meval = self._build_models()\n",
    "\n",
    "      # Figure out what epoch we are on\n",
    "      starting_epoch = self._calc_starting_epoch(m)\n",
    "\n",
    "      # Run the validation error right at the beginning\n",
    "      valid_accuracy = self.eval_child_model(\n",
    "          meval, self.data_loader, 'val')\n",
    "      tf.logging.info('Before Training Epoch: {}     Val Acc: {}'.format(\n",
    "          starting_epoch, valid_accuracy))\n",
    "      training_accuracy = None\n",
    "\n",
    "      for curr_epoch in xrange(starting_epoch, hparams.num_epochs):\n",
    "\n",
    "        # Run one training epoch\n",
    "        training_accuracy = self._run_training_loop(m, curr_epoch)\n",
    "\n",
    "        valid_accuracy = self.eval_child_model(\n",
    "            meval, self.data_loader, 'val')\n",
    "        tf.logging.info('Epoch: {}    Valid Acc: {}'.format(\n",
    "            curr_epoch, valid_accuracy))\n",
    "\n",
    "      valid_accuracy, test_accuracy = self._compute_final_accuracies(\n",
    "          meval)\n",
    "\n",
    "    tf.logging.info(\n",
    "        'Train Acc: {}    Valid Acc: {}     Test Acc: {}'.format(\n",
    "            training_accuracy, valid_accuracy, test_accuracy))\n",
    "\n",
    "  @property\n",
    "  def saver(self):\n",
    "    return self._saver\n",
    "\n",
    "  @property\n",
    "  def session(self):\n",
    "    return self._session\n",
    "\n",
    "  @property\n",
    "  def num_trainable_params(self):\n",
    "    return self._num_trainable_params\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  if FLAGS.dataset not in ['cifar10', 'cifar100']:\n",
    "    raise ValueError('Invalid dataset: %s' % FLAGS.dataset)\n",
    "  hparams = tf.contrib.training.HParams(\n",
    "      train_size=50000,\n",
    "      validation_size=0,\n",
    "      eval_test=1,\n",
    "      dataset=FLAGS.dataset,\n",
    "      data_path=FLAGS.data_path,\n",
    "      batch_size=128,\n",
    "      gradient_clipping_by_global_norm=5.0)\n",
    "  if FLAGS.model_name == 'wrn':\n",
    "    hparams.add_hparam('model_name', 'wrn')\n",
    "    hparams.add_hparam('num_epochs', 200)\n",
    "    hparams.add_hparam('wrn_size', 160)\n",
    "    hparams.add_hparam('lr', 0.1)\n",
    "    hparams.add_hparam('weight_decay_rate', 5e-4)\n",
    "  elif FLAGS.model_name == 'shake_shake_32':\n",
    "    hparams.add_hparam('model_name', 'shake_shake')\n",
    "    hparams.add_hparam('num_epochs', 1800)\n",
    "    hparams.add_hparam('shake_shake_widen_factor', 2)\n",
    "    hparams.add_hparam('lr', 0.01)\n",
    "    hparams.add_hparam('weight_decay_rate', 0.001)\n",
    "  elif FLAGS.model_name == 'shake_shake_96':\n",
    "    hparams.add_hparam('model_name', 'shake_shake')\n",
    "    hparams.add_hparam('num_epochs', 1800)\n",
    "    hparams.add_hparam('shake_shake_widen_factor', 6)\n",
    "    hparams.add_hparam('lr', 0.01)\n",
    "    hparams.add_hparam('weight_decay_rate', 0.001)\n",
    "  elif FLAGS.model_name == 'shake_shake_112':\n",
    "    hparams.add_hparam('model_name', 'shake_shake')\n",
    "    hparams.add_hparam('num_epochs', 1800)\n",
    "    hparams.add_hparam('shake_shake_widen_factor', 7)\n",
    "    hparams.add_hparam('lr', 0.01)\n",
    "    hparams.add_hparam('weight_decay_rate', 0.001)\n",
    "  elif FLAGS.model_name == 'pyramid_net':\n",
    "    hparams.add_hparam('model_name', 'pyramid_net')\n",
    "    hparams.add_hparam('num_epochs', 1800)\n",
    "    hparams.add_hparam('lr', 0.05)\n",
    "    hparams.add_hparam('weight_decay_rate', 5e-5)\n",
    "    hparams.batch_size = 64\n",
    "  else:\n",
    "    raise ValueError('Not Valid Model Name: %s' % FLAGS.model_name)\n",
    "  cifar_trainer = CifarModelTrainer(hparams)\n",
    "  cifar_trainer.run_model()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
